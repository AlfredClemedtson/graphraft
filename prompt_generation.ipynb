{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-05T16:09:27.523473Z",
     "start_time": "2025-02-05T16:09:26.385844Z"
    }
   },
   "source": [
    "from eval_llm2 import qa_with_eval_prompts\n",
    "from neo4j import GraphDatabase\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_from_disk, DatasetDict\n",
    "\n",
    "from cypher_parsing import cypher2path, path2cypher\n",
    "\n",
    "load_dotenv('db.env', override=True)\n",
    "NEO4J_URI = os.getenv('NEO4J_URI')\n",
    "NEO4J_USERNAME = os.getenv('NEO4J_USERNAME')\n",
    "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "NEO4J_URI"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alfred/miniconda3/envs/llm-experiment/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'bolt://localhost:7687'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T16:09:29.035223Z",
     "start_time": "2025-02-05T16:09:27.526333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = 'meta-llama/Llama-3.1-8B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ],
   "id": "276a728cb5f1f334",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T16:20:35.175909Z",
     "start_time": "2025-02-05T16:20:35.170548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def modified_query(cypher_query: str) -> str:\n",
    "    cypher_query = cypher_query.split(\" RETURN\")[0]\n",
    "    path = cypher2path(cypher_query)\n",
    "    _, last_num, _, last_name = path[-1]\n",
    "    tgt = f'x{last_num}' if not (last_num == 3 and last_name == '') else 'x2' #tgt is the last one, except for '2path'\n",
    "    return f\"\"\"{cypher_query} RETURN DISTINCT\n",
    "                                     {tgt}.name AS name, \n",
    "                                     {tgt}.details AS details, \n",
    "                                     {tgt}.nodeId AS node_id,\n",
    "                                     vector.similarity.cosine({tgt}.textEmbedding, $questionEmbedding) AS similarity\n",
    "                               ORDER BY similarity DESC\"\"\"\n",
    "\n",
    "def format_pattern(cypher_query: str, fetched_name: str) -> str:\n",
    "    path = cypher2path(cypher_query)\n",
    "    if len(path) == 1:\n",
    "        return \"Mentioned\"#cypher_query.split(\" RETURN\")[0]\n",
    "    x_r, num, label, name = path[-1]\n",
    "    if name == '': #src-tgt AND src-var-tgt\n",
    "        path[-1] = (x_r, num, label, fetched_name)\n",
    "    else: #src1-tgt-src2\n",
    "        try:\n",
    "            x_r, num, label, name = path[2]\n",
    "        except IndexError:\n",
    "            print(path)\n",
    "        path[2] = (x_r, num, label, fetched_name)\n",
    "    return path2cypher(path).lstrip(\"MATCH \")\n",
    "\n",
    "def make_description(rec: dict) -> (str, float):\n",
    "    name = rec['name']\n",
    "    patterns = rec.get('patterns', None)\n",
    "    details = rec['details']\n",
    "    if patterns is not None:\n",
    "        patterns_string = ', '.join([format_pattern(pattern, name) for pattern in patterns])\n",
    "    else:\n",
    "        patterns_string = \"No pattern\" #Idea: use cypher to find pattern for vector similar nodes!\n",
    "    return f\"\"\"Name: {name}\\nPatterns: {patterns_string}\\nDescription: {details}\"\"\"\n",
    "\n",
    "def data_fetcher(data, question_embedding, driver) -> ([tuple[dict,str], None, None]): #generator which produces db outputs\n",
    "    for db_query in data['db_queries']:\n",
    "        pattern = db_query.split(\" RETURN\")[0]\n",
    "        with driver.session() as session:\n",
    "            for rec in session.run(db_query, parameters={'questionEmbedding': question_embedding}):\n",
    "                yield rec, pattern\n",
    "                \n",
    "def data_fetcher_vector_sim(question_embedding: list[float], max_num_nodes: int, found_node_ids: list[int], driver) -> ([dict, None, None]):\n",
    "    ef = 5 * max_num_nodes\n",
    "    with driver.session() as session:\n",
    "        db_query = \"\"\"CALL db.index.vector.queryNodes('textEmbedding', $numNodes, $questionEmbedding) YIELD node AS node, score\n",
    "                      WHERE NOT node.nodeId IN $foundNodeIds\n",
    "                      RETURN node.name AS name, \n",
    "                             node.details AS details, \n",
    "                             node.nodeId AS node_id,\n",
    "                             score AS similarity\"\"\"\n",
    "        for rec in session.run(db_query, parameters={'numNodes': max_num_nodes, 'questionEmbedding': question_embedding, 'foundNodeIds': found_node_ids}):\n",
    "            yield rec\n",
    "        \n",
    "def get_answer_names(answer_ids: list[int]) -> list[str]:\n",
    "    with GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD)) as driver:\n",
    "        db_query = \"\"\"UNWIND $nodeIds AS nodeId \n",
    "                      MATCH (x:_Entity_ {nodeId: nodeId})\n",
    "                      RETURN x.name as name\"\"\"\n",
    "        res = driver.execute_query(db_query, {'nodeIds': answer_ids})\n",
    "        answer_names = [rec['name'] for rec in res.records]\n",
    "        return answer_names"
   ],
   "id": "b2c051934f5e6347",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T16:25:55.185519Z",
     "start_time": "2025-02-05T16:25:55.021454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MAX_TOKENS = 20_000\n",
    "#MAX_TOKENS = 100_000\n",
    "EXTRA_TOKENS_PER_NODE = 10 # To assure we don't hit the context-window max size\n",
    "CYPHER_RATE = .5 #Rest is taken by vector similarity\n",
    "\n",
    "q_embs = torch.load('prime-data/text-embeddings-ada-002/query/query_emb_dict.pt', weights_only=False)\n",
    "\n",
    "def add_retrieved_data(data: dict, driver: GraphDatabase.driver) -> dict:\n",
    "    idx = data['id']\n",
    "    question = data['question']\n",
    "    q_emb = q_embs[idx].tolist()[0]\n",
    "    \n",
    "    new_queries = [f\"\"\"MATCH (x1:_Entity_ {{name: \"{node_name}\"}})\"\"\" for node_name in data['predicted_entities']] #Add the source nodes to the result as well\n",
    "    data['db_queries'] = [modified_query(query) for query in new_queries+data['ordered_cypher_queries']]\n",
    "\n",
    "    answer_names = get_answer_names(eval(data['answer_ids']))\n",
    "    answer = '|'.join(answer_names)\n",
    "    \n",
    "    node_data = {}\n",
    "    num_tokens = len(tokenizer.tokenize(question)) + len(tokenizer.tokenize(answer)) + 10\n",
    "    for rec, cypher_query in data_fetcher(data, question_embedding=q_emb, driver=driver):\n",
    "        node_id = rec['node_id']\n",
    "        if node_id in node_data.keys(): #already found\n",
    "            num_new_tokens = len(tokenizer.tokenize(cypher_query))\n",
    "            if num_tokens + num_new_tokens > (1-CYPHER_RATE)*MAX_TOKENS:\n",
    "                break\n",
    "            node_data[node_id]['patterns'].append(cypher_query)\n",
    "        else:\n",
    "            num_new_tokens = len(tokenizer.tokenize(cypher_query)) + len(tokenizer.tokenize(rec['details'] if rec['details'] is not None else '')) + EXTRA_TOKENS_PER_NODE\n",
    "            if num_tokens + num_new_tokens > (1-CYPHER_RATE)*MAX_TOKENS:\n",
    "                break\n",
    "            node_data[node_id] = {'name': rec['name'], 'patterns': [cypher_query], 'details': rec['details'], 'similarity': rec['similarity']}\n",
    "        num_tokens += num_new_tokens\n",
    "        \n",
    "    # Order by similarity (most similar first) (but append vector-similar to the end)\n",
    "    node_ids = list(node_data.keys())\n",
    "    node_texts = [make_description(val) for val in sorted(node_data.values(), key=lambda x: x['similarity'], reverse=True)] \n",
    "    \n",
    "    \n",
    "    for rec in data_fetcher_vector_sim(q_emb, max_num_nodes=100, found_node_ids=node_ids, driver=driver):\n",
    "        node_text = make_description(rec)\n",
    "        num_new_tokens = len(tokenizer.tokenize(node_text)) + EXTRA_TOKENS_PER_NODE\n",
    "        if num_tokens + num_new_tokens > MAX_TOKENS:\n",
    "            break\n",
    "        else:\n",
    "            num_tokens += num_new_tokens\n",
    "            node_ids.append(rec['node_id'])\n",
    "            node_texts.append(node_text)\n",
    "    info = '\\n\\n'.join(node_texts)\n",
    "    data['info'] = info\n",
    "    data['info_nodes'] = node_ids\n",
    "    data['answer'] = answer\n",
    "    return data\n",
    "\n",
    "def sort_cyphers(data: dict) -> dict:\n",
    "    cyphers, hits, num_results = data['cyphers'], data['hits'], data['num_results']\n",
    "    data['ordered_cypher_queries'] = zip(*sorted(zip(cyphers, hits, num_results), key=lambda x: (-x[1],x[2])))\n",
    "    return data"
   ],
   "id": "54d29b3f316c6c20",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T16:25:56.214060Z",
     "start_time": "2025-02-05T16:25:55.822010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "qa_with_train_cyphers = load_from_disk('prime-data/qa_with_train_cyphers')\n",
    "\n",
    "qa_with_train_prompts = DatasetDict({'train' : qa_with_train_cyphers['train'], 'valid' : qa_with_train_cyphers['valid']})\n",
    "with GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD)) as driver:\n",
    "    qa_with_train_prompts = qa_with_train_prompts.map(sort_cyphers).map(lambda x: add_retrieved_data(x, driver), num_proc=8)\n",
    "qa_with_train_prompts.save_to_disk('prime-data/qa_with_train_prompts')"
   ],
   "id": "38b05b02707ca3ca",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  16%|█▌        | 999/6162 [00:00<00:01, 4363.02 examples/s]\n"
     ]
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "Could not convert <zip object at 0x344496000> with type zip: did not recognize Python value type when inferring an Arrow data type",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mArrowInvalid\u001B[0m                              Traceback (most recent call last)",
      "File \u001B[0;32m~/miniconda3/envs/llm-experiment/lib/python3.12/site-packages/datasets/arrow_writer.py:228\u001B[0m, in \u001B[0;36mTypedSequence.__arrow_array__\u001B[0;34m(self, type)\u001B[0m\n\u001B[1;32m    227\u001B[0m     trying_cast_to_python_objects \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m--> 228\u001B[0m     out \u001B[38;5;241m=\u001B[39m pa\u001B[38;5;241m.\u001B[39marray(cast_to_python_objects(data, only_1d_for_numpy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m))\n\u001B[1;32m    229\u001B[0m \u001B[38;5;66;03m# use smaller integer precisions if possible\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/llm-experiment/lib/python3.12/site-packages/pyarrow/array.pxi:370\u001B[0m, in \u001B[0;36mpyarrow.lib.array\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/miniconda3/envs/llm-experiment/lib/python3.12/site-packages/pyarrow/array.pxi:42\u001B[0m, in \u001B[0;36mpyarrow.lib._sequence_to_array\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/miniconda3/envs/llm-experiment/lib/python3.12/site-packages/pyarrow/error.pxi:155\u001B[0m, in \u001B[0;36mpyarrow.lib.pyarrow_internal_check_status\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/miniconda3/envs/llm-experiment/lib/python3.12/site-packages/pyarrow/error.pxi:92\u001B[0m, in \u001B[0;36mpyarrow.lib.check_status\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mArrowInvalid\u001B[0m: Could not convert <zip object at 0x344496000> with type zip: did not recognize Python value type when inferring an Arrow data type",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mArrowInvalid\u001B[0m                              Traceback (most recent call last)",
      "File \u001B[0;32m~/miniconda3/envs/llm-experiment/lib/python3.12/site-packages/datasets/arrow_dataset.py:3462\u001B[0m, in \u001B[0;36mDataset._map_single\u001B[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001B[0m\n\u001B[1;32m   3461\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 3462\u001B[0m         writer\u001B[38;5;241m.\u001B[39mwrite(example)\n\u001B[1;32m   3463\u001B[0m num_examples_progress_update \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m~/miniconda3/envs/llm-experiment/lib/python3.12/site-packages/datasets/arrow_writer.py:537\u001B[0m, in \u001B[0;36mArrowWriter.write\u001B[0;34m(self, example, key, writer_batch_size)\u001B[0m\n\u001B[1;32m    535\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhkey_record \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m--> 537\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwrite_examples_on_file()\n",
      "File \u001B[0;32m~/miniconda3/envs/llm-experiment/lib/python3.12/site-packages/datasets/arrow_writer.py:495\u001B[0m, in \u001B[0;36mArrowWriter.write_examples_on_file\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    491\u001B[0m         batch_examples[col] \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    492\u001B[0m             row[\u001B[38;5;241m0\u001B[39m][col]\u001B[38;5;241m.\u001B[39mto_pylist()[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(row[\u001B[38;5;241m0\u001B[39m][col], (pa\u001B[38;5;241m.\u001B[39mArray, pa\u001B[38;5;241m.\u001B[39mChunkedArray)) \u001B[38;5;28;01melse\u001B[39;00m row[\u001B[38;5;241m0\u001B[39m][col]\n\u001B[1;32m    493\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m row \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcurrent_examples\n\u001B[1;32m    494\u001B[0m         ]\n\u001B[0;32m--> 495\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwrite_batch(batch_examples\u001B[38;5;241m=\u001B[39mbatch_examples)\n\u001B[1;32m    496\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcurrent_examples \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[0;32m~/miniconda3/envs/llm-experiment/lib/python3.12/site-packages/datasets/arrow_writer.py:605\u001B[0m, in \u001B[0;36mArrowWriter.write_batch\u001B[0;34m(self, batch_examples, writer_batch_size)\u001B[0m\n\u001B[1;32m    604\u001B[0m typed_sequence \u001B[38;5;241m=\u001B[39m OptimizedTypedSequence(col_values, \u001B[38;5;28mtype\u001B[39m\u001B[38;5;241m=\u001B[39mcol_type, try_type\u001B[38;5;241m=\u001B[39mcol_try_type, col\u001B[38;5;241m=\u001B[39mcol)\n\u001B[0;32m--> 605\u001B[0m arrays\u001B[38;5;241m.\u001B[39mappend(pa\u001B[38;5;241m.\u001B[39marray(typed_sequence))\n\u001B[1;32m    606\u001B[0m inferred_features[col] \u001B[38;5;241m=\u001B[39m typed_sequence\u001B[38;5;241m.\u001B[39mget_inferred_type()\n",
      "File \u001B[0;32m~/miniconda3/envs/llm-experiment/lib/python3.12/site-packages/pyarrow/array.pxi:250\u001B[0m, in \u001B[0;36mpyarrow.lib.array\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/miniconda3/envs/llm-experiment/lib/python3.12/site-packages/pyarrow/array.pxi:114\u001B[0m, in \u001B[0;36mpyarrow.lib._handle_arrow_array_protocol\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/miniconda3/envs/llm-experiment/lib/python3.12/site-packages/datasets/arrow_writer.py:296\u001B[0m, in \u001B[0;36mTypedSequence.__arrow_array__\u001B[0;34m(self, type)\u001B[0m\n\u001B[1;32m    295\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m trying_cast_to_python_objects \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not convert\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(e):\n\u001B[0;32m--> 296\u001B[0m     out \u001B[38;5;241m=\u001B[39m pa\u001B[38;5;241m.\u001B[39marray(cast_to_python_objects(data, only_1d_for_numpy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, optimize_list_casting\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m))\n\u001B[1;32m    297\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/miniconda3/envs/llm-experiment/lib/python3.12/site-packages/pyarrow/array.pxi:370\u001B[0m, in \u001B[0;36mpyarrow.lib.array\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/miniconda3/envs/llm-experiment/lib/python3.12/site-packages/pyarrow/array.pxi:42\u001B[0m, in \u001B[0;36mpyarrow.lib._sequence_to_array\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/miniconda3/envs/llm-experiment/lib/python3.12/site-packages/pyarrow/error.pxi:155\u001B[0m, in \u001B[0;36mpyarrow.lib.pyarrow_internal_check_status\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/miniconda3/envs/llm-experiment/lib/python3.12/site-packages/pyarrow/error.pxi:92\u001B[0m, in \u001B[0;36mpyarrow.lib.check_status\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mArrowInvalid\u001B[0m: Could not convert <zip object at 0x344496000> with type zip: did not recognize Python value type when inferring an Arrow data type",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mArrowInvalid\u001B[0m                              Traceback (most recent call last)",
      "File \u001B[0;32m~/miniconda3/envs/llm-experiment/lib/python3.12/site-packages/datasets/arrow_writer.py:228\u001B[0m, in \u001B[0;36mTypedSequence.__arrow_array__\u001B[0;34m(self, type)\u001B[0m\n\u001B[1;32m    227\u001B[0m     trying_cast_to_python_objects \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m--> 228\u001B[0m     out \u001B[38;5;241m=\u001B[39m pa\u001B[38;5;241m.\u001B[39marray(cast_to_python_objects(data, only_1d_for_numpy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m))\n\u001B[1;32m    229\u001B[0m \u001B[38;5;66;03m# use smaller integer precisions if possible\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/llm-experiment/lib/python3.12/site-packages/pyarrow/array.pxi:370\u001B[0m, in \u001B[0;36mpyarrow.lib.array\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/miniconda3/envs/llm-experiment/lib/python3.12/site-packages/pyarrow/array.pxi:42\u001B[0m, in \u001B[0;36mpyarrow.lib._sequence_to_array\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/miniconda3/envs/llm-experiment/lib/python3.12/site-packages/pyarrow/error.pxi:155\u001B[0m, in \u001B[0;36mpyarrow.lib.pyarrow_internal_check_status\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/miniconda3/envs/llm-experiment/lib/python3.12/site-packages/pyarrow/error.pxi:92\u001B[0m, in \u001B[0;36mpyarrow.lib.check_status\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mArrowInvalid\u001B[0m: Could not convert <zip object at 0x344496000> with type zip: did not recognize Python value type when inferring an Arrow data type",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mArrowInvalid\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[38], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m qa_with_train_prompts \u001B[38;5;241m=\u001B[39m DatasetDict({\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m : qa_with_train_cyphers[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalid\u001B[39m\u001B[38;5;124m'\u001B[39m : qa_with_train_cyphers[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalid\u001B[39m\u001B[38;5;124m'\u001B[39m]})\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m GraphDatabase\u001B[38;5;241m.\u001B[39mdriver(NEO4J_URI, auth\u001B[38;5;241m=\u001B[39m(NEO4J_USERNAME, NEO4J_PASSWORD)) \u001B[38;5;28;01mas\u001B[39;00m driver:\n\u001B[0;32m----> 5\u001B[0m     qa_with_train_prompts \u001B[38;5;241m=\u001B[39m qa_with_train_prompts\u001B[38;5;241m.\u001B[39mmap(sort_cyphers)\u001B[38;5;241m.\u001B[39mmap(\u001B[38;5;28;01mlambda\u001B[39;00m x: add_retrieved_data(x, driver), num_proc\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8\u001B[39m)\n\u001B[1;32m      6\u001B[0m qa_with_train_prompts\u001B[38;5;241m.\u001B[39msave_to_disk(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprime-data/qa_with_train_prompts\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/llm-experiment/lib/python3.12/site-packages/datasets/dataset_dict.py:887\u001B[0m, in \u001B[0;36mDatasetDict.map\u001B[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001B[0m\n\u001B[1;32m    883\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cache_file_names \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    884\u001B[0m     cache_file_names \u001B[38;5;241m=\u001B[39m {k: \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m}\n\u001B[1;32m    885\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DatasetDict(\n\u001B[1;32m    886\u001B[0m     {\n\u001B[0;32m--> 887\u001B[0m         k: dataset\u001B[38;5;241m.\u001B[39mmap(\n\u001B[1;32m    888\u001B[0m             function\u001B[38;5;241m=\u001B[39mfunction,\n\u001B[1;32m    889\u001B[0m             with_indices\u001B[38;5;241m=\u001B[39mwith_indices,\n\u001B[1;32m    890\u001B[0m             with_rank\u001B[38;5;241m=\u001B[39mwith_rank,\n\u001B[1;32m    891\u001B[0m             input_columns\u001B[38;5;241m=\u001B[39minput_columns,\n\u001B[1;32m    892\u001B[0m             batched\u001B[38;5;241m=\u001B[39mbatched,\n\u001B[1;32m    893\u001B[0m             batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[1;32m    894\u001B[0m             drop_last_batch\u001B[38;5;241m=\u001B[39mdrop_last_batch,\n\u001B[1;32m    895\u001B[0m             remove_columns\u001B[38;5;241m=\u001B[39mremove_columns,\n\u001B[1;32m    896\u001B[0m             keep_in_memory\u001B[38;5;241m=\u001B[39mkeep_in_memory,\n\u001B[1;32m    897\u001B[0m             load_from_cache_file\u001B[38;5;241m=\u001B[39mload_from_cache_file,\n\u001B[1;32m    898\u001B[0m             cache_file_name\u001B[38;5;241m=\u001B[39mcache_file_names[k],\n\u001B[1;32m    899\u001B[0m             writer_batch_size\u001B[38;5;241m=\u001B[39mwriter_batch_size,\n\u001B[1;32m    900\u001B[0m             features\u001B[38;5;241m=\u001B[39mfeatures,\n\u001B[1;32m    901\u001B[0m             disable_nullable\u001B[38;5;241m=\u001B[39mdisable_nullable,\n\u001B[1;32m    902\u001B[0m             fn_kwargs\u001B[38;5;241m=\u001B[39mfn_kwargs,\n\u001B[1;32m    903\u001B[0m             num_proc\u001B[38;5;241m=\u001B[39mnum_proc,\n\u001B[1;32m    904\u001B[0m             desc\u001B[38;5;241m=\u001B[39mdesc,\n\u001B[1;32m    905\u001B[0m         )\n\u001B[1;32m    906\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m k, dataset \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems()\n\u001B[1;32m    907\u001B[0m     }\n\u001B[1;32m    908\u001B[0m )\n",
      "File \u001B[0;32m~/miniconda3/envs/llm-experiment/lib/python3.12/site-packages/datasets/arrow_dataset.py:560\u001B[0m, in \u001B[0;36mtransmit_format.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    553\u001B[0m self_format \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    554\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_type,\n\u001B[1;32m    555\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mformat_kwargs\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_kwargs,\n\u001B[1;32m    556\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_columns,\n\u001B[1;32m    557\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_all_columns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_all_columns,\n\u001B[1;32m    558\u001B[0m }\n\u001B[1;32m    559\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[0;32m--> 560\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    561\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[1;32m    562\u001B[0m \u001B[38;5;66;03m# re-apply format to the output\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/llm-experiment/lib/python3.12/site-packages/datasets/arrow_dataset.py:3073\u001B[0m, in \u001B[0;36mDataset.map\u001B[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001B[0m\n\u001B[1;32m   3067\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m transformed_dataset \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   3068\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m hf_tqdm(\n\u001B[1;32m   3069\u001B[0m         unit\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m examples\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   3070\u001B[0m         total\u001B[38;5;241m=\u001B[39mpbar_total,\n\u001B[1;32m   3071\u001B[0m         desc\u001B[38;5;241m=\u001B[39mdesc \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMap\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   3072\u001B[0m     ) \u001B[38;5;28;01mas\u001B[39;00m pbar:\n\u001B[0;32m-> 3073\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m rank, done, content \u001B[38;5;129;01min\u001B[39;00m Dataset\u001B[38;5;241m.\u001B[39m_map_single(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdataset_kwargs):\n\u001B[1;32m   3074\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m done:\n\u001B[1;32m   3075\u001B[0m                 shards_done \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m~/miniconda3/envs/llm-experiment/lib/python3.12/site-packages/datasets/arrow_dataset.py:3511\u001B[0m, in \u001B[0;36mDataset._map_single\u001B[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001B[0m\n\u001B[1;32m   3509\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m update_data:\n\u001B[1;32m   3510\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m writer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 3511\u001B[0m         writer\u001B[38;5;241m.\u001B[39mfinalize()\n\u001B[1;32m   3512\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m tmp_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   3513\u001B[0m         tmp_file\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[0;32m~/miniconda3/envs/llm-experiment/lib/python3.12/site-packages/datasets/arrow_writer.py:636\u001B[0m, in \u001B[0;36mArrowWriter.finalize\u001B[0;34m(self, close_stream)\u001B[0m\n\u001B[1;32m    634\u001B[0m     \u001B[38;5;66;03m# Re-intializing to empty list for next batch\u001B[39;00m\n\u001B[1;32m    635\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhkey_record \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m--> 636\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwrite_examples_on_file()\n\u001B[1;32m    637\u001B[0m \u001B[38;5;66;03m# If schema is known, infer features even if no examples were written\u001B[39;00m\n\u001B[1;32m    638\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpa_writer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mschema:\n",
      "File \u001B[0;32m~/miniconda3/envs/llm-experiment/lib/python3.12/site-packages/datasets/arrow_writer.py:495\u001B[0m, in \u001B[0;36mArrowWriter.write_examples_on_file\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    490\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    491\u001B[0m         batch_examples[col] \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    492\u001B[0m             row[\u001B[38;5;241m0\u001B[39m][col]\u001B[38;5;241m.\u001B[39mto_pylist()[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(row[\u001B[38;5;241m0\u001B[39m][col], (pa\u001B[38;5;241m.\u001B[39mArray, pa\u001B[38;5;241m.\u001B[39mChunkedArray)) \u001B[38;5;28;01melse\u001B[39;00m row[\u001B[38;5;241m0\u001B[39m][col]\n\u001B[1;32m    493\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m row \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcurrent_examples\n\u001B[1;32m    494\u001B[0m         ]\n\u001B[0;32m--> 495\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwrite_batch(batch_examples\u001B[38;5;241m=\u001B[39mbatch_examples)\n\u001B[1;32m    496\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcurrent_examples \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[0;32m~/miniconda3/envs/llm-experiment/lib/python3.12/site-packages/datasets/arrow_writer.py:605\u001B[0m, in \u001B[0;36mArrowWriter.write_batch\u001B[0;34m(self, batch_examples, writer_batch_size)\u001B[0m\n\u001B[1;32m    603\u001B[0m         col_try_type \u001B[38;5;241m=\u001B[39m try_features[col] \u001B[38;5;28;01mif\u001B[39;00m try_features \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m col \u001B[38;5;129;01min\u001B[39;00m try_features \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    604\u001B[0m         typed_sequence \u001B[38;5;241m=\u001B[39m OptimizedTypedSequence(col_values, \u001B[38;5;28mtype\u001B[39m\u001B[38;5;241m=\u001B[39mcol_type, try_type\u001B[38;5;241m=\u001B[39mcol_try_type, col\u001B[38;5;241m=\u001B[39mcol)\n\u001B[0;32m--> 605\u001B[0m         arrays\u001B[38;5;241m.\u001B[39mappend(pa\u001B[38;5;241m.\u001B[39marray(typed_sequence))\n\u001B[1;32m    606\u001B[0m         inferred_features[col] \u001B[38;5;241m=\u001B[39m typed_sequence\u001B[38;5;241m.\u001B[39mget_inferred_type()\n\u001B[1;32m    607\u001B[0m schema \u001B[38;5;241m=\u001B[39m inferred_features\u001B[38;5;241m.\u001B[39marrow_schema \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpa_writer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mschema\n",
      "File \u001B[0;32m~/miniconda3/envs/llm-experiment/lib/python3.12/site-packages/pyarrow/array.pxi:250\u001B[0m, in \u001B[0;36mpyarrow.lib.array\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/miniconda3/envs/llm-experiment/lib/python3.12/site-packages/pyarrow/array.pxi:114\u001B[0m, in \u001B[0;36mpyarrow.lib._handle_arrow_array_protocol\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/miniconda3/envs/llm-experiment/lib/python3.12/site-packages/datasets/arrow_writer.py:296\u001B[0m, in \u001B[0;36mTypedSequence.__arrow_array__\u001B[0;34m(self, type)\u001B[0m\n\u001B[1;32m    294\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m out\n\u001B[1;32m    295\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m trying_cast_to_python_objects \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not convert\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(e):\n\u001B[0;32m--> 296\u001B[0m     out \u001B[38;5;241m=\u001B[39m pa\u001B[38;5;241m.\u001B[39marray(cast_to_python_objects(data, only_1d_for_numpy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, optimize_list_casting\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m))\n\u001B[1;32m    297\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    298\u001B[0m         out \u001B[38;5;241m=\u001B[39m cast_array_to_feature(out, \u001B[38;5;28mtype\u001B[39m, allow_primitive_to_str\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, allow_decimal_to_str\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m~/miniconda3/envs/llm-experiment/lib/python3.12/site-packages/pyarrow/array.pxi:370\u001B[0m, in \u001B[0;36mpyarrow.lib.array\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/miniconda3/envs/llm-experiment/lib/python3.12/site-packages/pyarrow/array.pxi:42\u001B[0m, in \u001B[0;36mpyarrow.lib._sequence_to_array\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/miniconda3/envs/llm-experiment/lib/python3.12/site-packages/pyarrow/error.pxi:155\u001B[0m, in \u001B[0;36mpyarrow.lib.pyarrow_internal_check_status\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/miniconda3/envs/llm-experiment/lib/python3.12/site-packages/pyarrow/error.pxi:92\u001B[0m, in \u001B[0;36mpyarrow.lib.check_status\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mArrowInvalid\u001B[0m: Could not convert <zip object at 0x344496000> with type zip: did not recognize Python value type when inferring an Arrow data type"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T16:26:54.604375Z",
     "start_time": "2025-02-05T16:26:30.250314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "qa_with_gen_cyphers = load_from_disk('prime-data/qa_with_pred_cyphers')\n",
    "\n",
    "qa_with_eval_prompts = DatasetDict({'valid' : qa_with_gen_cyphers['valid']})#, 'test' : qa_with_gen_cyphers['test']})\n",
    "qa_with_eval_prompts = qa_with_eval_prompts.map(lambda x: x | {'ordered_cypher_queries': x['cypher_preds']})\n",
    "\n",
    "with GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD)) as driver:\n",
    "    qa_with_eval_prompts = qa_with_eval_prompts.map(lambda x: add_retrieved_data(x, driver), num_proc=8)\n",
    "\n",
    "#qa_with_eval_prompts.save_to_disk('prime-data/qa_with_eval_prompts')"
   ],
   "id": "62ef12997a2f9c15",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 225/225 [00:00<00:00, 15692.29 examples/s]\n",
      "Map (num_proc=8): 100%|██████████| 225/225 [00:21<00:00, 10.25 examples/s]\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T16:29:54.193730Z",
     "start_time": "2025-02-05T16:29:54.149924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate prompts by recall etc.\n",
    "import numpy as np\n",
    "precs = []; recs = []; f1s = []; num_nodes = []\n",
    "#for qa in load_from_disk('prime-data/qa_with_train_cyphers')['train']:\n",
    "for qa in qa_with_eval_prompts['valid']:\n",
    "    answer_nodes = eval(qa['answer_ids'])\n",
    "    prompt_nodes = qa['info_nodes']\n",
    "    hits = len(set(answer_nodes).intersection(prompt_nodes))\n",
    "    prec = hits/len(prompt_nodes) if len(prompt_nodes) > 0 else 0\n",
    "    rec = hits/len(answer_nodes)\n",
    "    f1 = (2*prec*rec)/(prec+rec) if hits > 0 else 0\n",
    "    precs.append(prec)\n",
    "    recs.append(rec)\n",
    "    f1s.append(f1)\n",
    "    num_nodes.append(len(prompt_nodes))\n",
    "print(f\"Avg prec:   {np.mean(precs):.3f}\\nAvg rec:    {np.mean(recs):.3f}\\nAvg f1:     {np.mean(f1s):.3f}\\nAvg #nodes: {np.mean(num_nodes):.1f}\\nMed #nodes: {np.median(num_nodes):.1f}\")"
   ],
   "id": "68da2461ae3f81d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg prec:   0.035\n",
      "Avg rec:    0.714\n",
      "Avg f1:     0.062\n",
      "Avg #nodes: 65.9\n",
      "Med #nodes: 50.0\n"
     ]
    }
   ],
   "execution_count": 41
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
