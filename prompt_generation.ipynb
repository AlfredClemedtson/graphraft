{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-24T10:52:16.266286Z",
     "start_time": "2025-02-24T10:52:16.263414Z"
    }
   },
   "source": [
    "from neo4j import GraphDatabase\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "from cypher_parsing import cypher2path, path2cypher\n",
    "import numpy as np\n",
    "\n",
    "load_dotenv('db.env', override=True)\n",
    "NEO4J_URI = os.getenv('NEO4J_URI')\n",
    "NEO4J_USERNAME = os.getenv('NEO4J_USERNAME')\n",
    "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "NEO4J_URI"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bolt://localhost:7687'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 176
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T10:21:45.907781Z",
     "start_time": "2025-02-21T10:21:43.827316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = 'meta-llama/Llama-3.1-8B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ],
   "id": "276a728cb5f1f334",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def modified_query(cypher_query: str) -> str:\n",
    "    cypher_query = cypher_query.split(\" RETURN\")[0]\n",
    "    path = cypher2path(cypher_query)\n",
    "    _, last_num, _, last_name = path[-1]\n",
    "    tgt = f'x{last_num}' if not (last_num == 3 and last_name == '') else 'x2' #tgt is the last one, except for '2path'\n",
    "    return f\"\"\"{cypher_query} RETURN DISTINCT\n",
    "                                     {tgt}.name AS name, \n",
    "                                     {tgt}.details AS details, \n",
    "                                     {tgt}.nodeId AS node_id,\n",
    "                                     vector.similarity.cosine({tgt}.textEmbedding, $questionEmbedding) AS similarity\n",
    "                               ORDER BY similarity DESC\"\"\"\n",
    "\n",
    "def format_pattern(cypher_query: str, fetched_name: str) -> str:\n",
    "    path = cypher2path(cypher_query)\n",
    "    if len(path) == 1:\n",
    "        return \"Mentioned\"#cypher_query.split(\" RETURN\")[0]\n",
    "    x_r, num, label, name = path[-1]\n",
    "    if name == '': #src-tgt AND src-var-tgt\n",
    "        path[-1] = (x_r, num, label, fetched_name)\n",
    "    else: #src1-tgt-src2\n",
    "        try:\n",
    "            x_r, num, label, name = path[2]\n",
    "        except IndexError:\n",
    "            print(path)\n",
    "        path[2] = (x_r, num, label, fetched_name)\n",
    "    return path2cypher(path).lstrip(\"MATCH \")\n",
    "\n",
    "def make_description(rec: dict) -> (str, float):\n",
    "    name = rec['name']\n",
    "    patterns = rec.get('patterns', None)\n",
    "    details = rec['details']\n",
    "    if patterns is not None:\n",
    "        patterns_string = ', '.join([format_pattern(pattern, name) for pattern in patterns])\n",
    "    else:\n",
    "        patterns_string = \"No pattern\" #Idea: use cypher to find pattern for vector similar nodes!\n",
    "    return f\"\"\"Name: {name}\\nPatterns: {patterns_string}\\nDescription: {details}\"\"\"\n",
    "\n",
    "def data_fetcher(data, question_embedding, driver) -> ([tuple[dict,str], None, None]): #generator which produces db outputs\n",
    "    for db_query in data['db_queries']:\n",
    "        pattern = db_query.split(\" RETURN\")[0]\n",
    "        with driver.session() as session:\n",
    "            try:\n",
    "                for rec in session.run(db_query, parameters={'questionEmbedding': question_embedding}):\n",
    "                    yield rec, pattern\n",
    "            except:\n",
    "                return \n",
    "                yield\n",
    "                \n",
    "def data_fetcher_vector_sim(question_embedding: list[float], max_num_nodes: int, found_node_ids: list[int], driver) -> ([dict, None, None]):\n",
    "    ef = 10 * max_num_nodes\n",
    "    with driver.session() as session:\n",
    "        db_query = \"\"\"CALL db.index.vector.queryNodes('textEmbedding', $ef, $questionEmbedding) YIELD node AS node, score\n",
    "                      WHERE NOT node.nodeId IN $foundNodeIds\n",
    "                      RETURN node.name AS name, \n",
    "                             node.details AS details, \n",
    "                             node.nodeId AS node_id,\n",
    "                             score AS similarity\n",
    "                      LIMIT $numNodes\"\"\"\n",
    "        for rec in session.run(db_query, parameters={'ef': ef, 'numNodes': max_num_nodes, 'questionEmbedding': question_embedding, 'foundNodeIds': found_node_ids}):\n",
    "            yield rec\n",
    "        \n",
    "def get_answer_names(answer_ids: list[int]) -> list[str]:\n",
    "    with GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD)) as driver:\n",
    "        db_query = \"\"\"UNWIND $nodeIds AS nodeId \n",
    "                      MATCH (x:_Entity_ {nodeId: nodeId})\n",
    "                      RETURN x.name as name\"\"\"\n",
    "        res = driver.execute_query(db_query, {'nodeIds': answer_ids})\n",
    "        answer_names = [rec['name'] for rec in res.records]\n",
    "        return answer_names"
   ],
   "id": "cbcdab1112df1580"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T14:15:27.472364Z",
     "start_time": "2025-02-24T14:15:27.317707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MAX_NUM_NODES = 20\n",
    "MAX_TOKENS = 10_000\n",
    "COUNTING_TOKENS = True#False\n",
    "EXTRA_TOKENS_PER_NODE = 10 # To assure we don't hit the context-window max size\n",
    "CYPHER_RATE = 1#.5 #Rest is taken by vector similarity\n",
    "\n",
    "q_embs = torch.load('prime-data/text-embeddings-ada-002/query/query_emb_dict.pt', weights_only=False)\n",
    "\n",
    "def stop(node_data, using_vector_search, num_tokens, num_new_tokens):\n",
    "    if not using_vector_search:\n",
    "        if COUNTING_TOKENS:\n",
    "            return num_tokens + num_new_tokens > CYPHER_RATE*MAX_TOKENS\n",
    "        else:\n",
    "            return len(node_data) >= CYPHER_RATE*MAX_NUM_NODES\n",
    "    else:\n",
    "        if COUNTING_TOKENS:\n",
    "            return num_tokens + num_new_tokens > MAX_TOKENS\n",
    "        else:\n",
    "            return len(node_data) >= MAX_NUM_NODES\n",
    "\n",
    "def add_retrieved_data(data: dict, driver: GraphDatabase.driver) -> dict:\n",
    "    idx = data['id']\n",
    "    question = data['question']\n",
    "    q_emb = q_embs[idx].tolist()[0]\n",
    "    \n",
    "    new_queries = [f\"\"\"MATCH (x1:_Entity_ {{name: \"{node_name}\"}})\"\"\" for node_name in data['predicted_entities']] #Add the source nodes to the result as well\n",
    "    data['db_queries'] = [modified_query(query) for query in new_queries+data['top_cypher_queries']]\n",
    "\n",
    "    answer_names = get_answer_names(data['answer_ids'])\n",
    "    answer = '|'.join(answer_names)\n",
    "    \n",
    "    node_data = {}\n",
    "    num_tokens = 100 + len(tokenizer.tokenize(question)) + len(tokenizer.tokenize(answer))\n",
    "    for rec, cypher_query in data_fetcher(data, question_embedding=q_emb, driver=driver):\n",
    "        node_id = rec['node_id']\n",
    "        if node_id in node_data.keys(): #already found\n",
    "            num_new_tokens = len(tokenizer.tokenize(cypher_query))\n",
    "            if stop(node_data, using_vector_search=False, num_tokens=num_tokens, num_new_tokens=num_new_tokens):\n",
    "                break\n",
    "        else:\n",
    "            num_new_tokens = len(tokenizer.tokenize(cypher_query)) + len(tokenizer.tokenize(rec['details'] if rec['details'] is not None else '')) + EXTRA_TOKENS_PER_NODE\n",
    "            if stop(node_data, using_vector_search=False, num_tokens=num_tokens, num_new_tokens=num_new_tokens):\n",
    "                break\n",
    "            node_data[node_id] = {'name': rec['name'], 'patterns': [cypher_query], 'details': rec['details'], 'similarity': rec['similarity']}\n",
    "        num_tokens += num_new_tokens\n",
    "        \n",
    "    # Order by similarity (most similar first) (but append vector-similar to the end)\n",
    "    node_ids = list(node_data.keys())\n",
    "    #node_ids, _ = zip(*sorted(zip(node_ids, node_data.values()), key=lambda x: x[1]['similarity'], reverse=True))  \n",
    "    node_texts = [make_description(val) for val in sorted(node_data.values(), key=lambda x: x['similarity'], reverse=True)] \n",
    "    \n",
    "    for rec in data_fetcher_vector_sim(q_emb, max_num_nodes=100, found_node_ids=node_ids, driver=driver):\n",
    "        node_text = make_description(rec)\n",
    "        num_new_tokens = len(tokenizer.tokenize(node_text)) + EXTRA_TOKENS_PER_NODE\n",
    "        if stop(node_ids, using_vector_search=True, num_tokens=num_tokens, num_new_tokens=num_new_tokens):\n",
    "            break\n",
    "        else:\n",
    "            num_tokens += num_new_tokens\n",
    "            node_ids.append(rec['node_id'])\n",
    "            node_texts.append(node_text)\n",
    "    info = '\\n\\n'.join(node_texts)\n",
    "    data['info'] = info\n",
    "    data['info_nodes'] = node_ids\n",
    "    data['answer'] = answer\n",
    "    return data\n",
    "\n",
    "def sort_cyphers(data: dict) -> dict:\n",
    "    cyphers, hits, num_results = data['cyphers'], data['hits'], data['num_results']\n",
    "    ordered_cypher_queries, _, _ = zip(*sorted(zip(cyphers, hits, num_results), key=lambda x: (-x[1],x[2])))\n",
    "    return ordered_cypher_queries\n",
    "    #data['ordered_cypher_queries'], _, _ = \n",
    "    #return data"
   ],
   "id": "54d29b3f316c6c20",
   "outputs": [],
   "execution_count": 385
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T15:27:23.721724Z",
     "start_time": "2025-02-25T15:27:23.718573Z"
    }
   },
   "cell_type": "code",
   "source": "sample(5, 27, alpha=0.1)",
   "id": "1565b981a6116be8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 11, 15, 4, 3]"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 450
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T15:33:05.047435Z",
     "start_time": "2025-02-25T15:33:05.045354Z"
    }
   },
   "cell_type": "code",
   "source": "10 * 6100/2800",
   "id": "86c030f415a8f41",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.785714285714285"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 452
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T09:11:07.543145Z",
     "start_time": "2025-02-27T09:01:55.787442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "def sample(num, max_idx, alpha):\n",
    "    inv_cdf = lambda x: x**(1/alpha)\n",
    "    samples = []\n",
    "    while len(samples) < num:\n",
    "        x = random.uniform(0,1)\n",
    "        rank = int(max_idx * inv_cdf(x))\n",
    "        if rank not in samples:\n",
    "            samples.append(rank)\n",
    "    return samples\n",
    "\n",
    "def add_relative_ranks(data: dict) -> list[int]:\n",
    "    true_ordered_cyphers = sort_cyphers(data)\n",
    "    llm_ordered_cyphers = data['top_cypher_queries']\n",
    "    data['relative_ranks'] = [true_ordered_cyphers.index(cypher)/len(true_ordered_cyphers) for cypher in llm_ordered_cyphers if cypher in true_ordered_cyphers]\n",
    "    return data\n",
    "\n",
    "def flatten(xss) -> list:\n",
    "    return [x for xs in xss for x in xs]\n",
    "\n",
    "def add_sampled_cypher_queries(data: dict, num_samples: int, alpha: float) -> dict:\n",
    "    true_ordered_cyphers = sort_cyphers(data)\n",
    "    max_idx = len(true_ordered_cyphers)\n",
    "    num_samples = min(num_samples, max_idx)\n",
    "    ids = sample(num=num_samples, max_idx=max_idx, alpha=alpha)\n",
    "    data['top_cypher_queries'] = [true_ordered_cyphers[idx] for idx in ids]\n",
    "    return data\n",
    "\n",
    "\n",
    "relative_ranks = load_from_disk('prime-data/qa_with_eval_cyphers').map(add_relative_ranks)['relative_ranks']\n",
    "mean = np.mean(flatten(relative_ranks)).item()\n",
    "#alpha = mean/(1-mean)\n",
    "alpha = 0.1\n",
    "\n",
    "#Add sampled queries\n",
    "qa_with_train_queries = load_from_disk('prime-data/qa_with_cyphers').map(lambda x: add_sampled_cypher_queries(x, num_samples=5, alpha=alpha))\n",
    "with (GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD)) as driver):\n",
    "    n = float('inf')\n",
    "    qa_with_train_prompts = qa_with_train_queries\\\n",
    "        .filter(lambda _,i: i < n, with_indices=True)\\\n",
    "        .map(lambda x: add_retrieved_data(x, driver), num_proc=8)\n",
    "qa_with_train_prompts.save_to_disk('prime-data/qa_with_train_prompts')\n",
    "#qa_with_train_prompts.save_to_disk('prime-data/qa_with_valid_prompts_sampled')"
   ],
   "id": "aa863ec99362a14b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2241/2241 [00:00<00:00, 7710.38 examples/s]\n",
      "Filter: 100%|██████████| 2241/2241 [00:00<00:00, 20922.27 examples/s]\n",
      "Map (num_proc=8): 100%|██████████| 2241/2241 [09:08<00:00,  4.08 examples/s]  \n",
      "Saving the dataset (1/1 shards): 100%|██████████| 2241/2241 [00:00<00:00, 45821.62 examples/s]\n"
     ]
    }
   ],
   "execution_count": 486
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T14:45:51.005108Z",
     "start_time": "2025-02-24T14:42:56.790906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#qa_with_eval_cyphers = load_from_disk('prime-data/qa_with_gen_cyphers').map(lambda x: x | {'top_cypher_queries': x['top_cyphers']})\n",
    "#qa_with_eval_cyphers = load_from_disk('prime-data/qa_with_pred_cyphers')['valid'].map(lambda x: x | {'ordered_cypher_queries': x['cypher_preds']})\n",
    "#qa_with_eval_cyphers = load_from_disk('prime-data/qa_with_cyphers')['valid'].map(lambda x: x | {'ordered_cypher_queries': x['cyphers']})\n",
    "#qa_with_eval_cyphers = load_from_disk('prime-data/qa_with_eval_cyphers_new_gemma').map(lambda x: x | {'ordered_cypher_queries': x['top_cyphers']})\n",
    "with (GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD)) as driver):\n",
    "    n = float('inf')\n",
    "    qa_with_eval_prompts = qa_with_eval_cyphers\\\n",
    "        .filter(lambda _,i: i < n, with_indices=True)\\\n",
    "        .map(lambda x: add_retrieved_data(x, driver), num_proc=8)\n",
    "qa_with_eval_prompts.save_to_disk('prime-data/qa_with_test_prompts')"
   ],
   "id": "8b4af2fc82919df2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2801/2801 [00:00<00:00, 20829.45 examples/s]\n",
      "Filter: 100%|██████████| 2801/2801 [00:00<00:00, 37401.57 examples/s]\n",
      "Map (num_proc=8): 100%|██████████| 2801/2801 [02:51<00:00, 16.36 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 2801/2801 [00:00<00:00, 51089.10 examples/s]\n"
     ]
    }
   ],
   "execution_count": 429
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T15:49:25.810535Z",
     "start_time": "2025-02-21T15:49:25.744349Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 2801/2801 [00:00<00:00, 45650.85 examples/s]\n"
     ]
    }
   ],
   "execution_count": 50,
   "source": "#qa_with_eval_prompts.save_to_disk('prime-data/qa_with_gen_prompts_test')",
   "id": "6d036f0a83aff9c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T14:50:39.325898Z",
     "start_time": "2025-02-24T14:49:45.697321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate prompts by recall etc.\n",
    "import numpy as np\n",
    "precs = []; recs = []; f1s = []\n",
    "num_nodes = []; num_tokens = []\n",
    "#for qa in load_from_disk('prime-data/qa_with_train_cyphers')['train']:\n",
    "for qa in tqdm(qa_with_train_prompts):#['valid']:\n",
    "    answer_nodes = qa['answer_ids']\n",
    "    prompt_nodes = qa['info_nodes']\n",
    "    hits = len(set(answer_nodes).intersection(prompt_nodes))\n",
    "    prec = hits/len(prompt_nodes) if len(prompt_nodes) > 0 else 0\n",
    "    rec = hits/len(answer_nodes)\n",
    "    f1 = (2*prec*rec)/(prec+rec) if hits > 0 else 0\n",
    "    precs.append(prec)\n",
    "    recs.append(rec)\n",
    "    f1s.append(f1)\n",
    "    num_tokens.append(len(tokenizer.tokenize(qa['info'])))\n",
    "    num_nodes.append(len(prompt_nodes))\n",
    "print(f\"Avg prec:    {np.mean(precs):.3f}\\nAvg rec:     {np.mean(recs):.3f}\\nAvg f1:      {np.mean(f1s):.3f}\\n\"\n",
    "      f\"Avg #nodes:  {np.mean(num_nodes):.1f}\\nMed #nodes:   {np.median(num_nodes):.1f}\\n\"\n",
    "      f\"Avg #tokens: {np.mean(num_tokens):.1f}\\nMed #tokens: {np.median(num_tokens):.1f}\")"
   ],
   "id": "68da2461ae3f81d0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6162/6162 [00:53<00:00, 114.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg prec:    0.069\n",
      "Avg rec:     0.644\n",
      "Avg f1:      0.106\n",
      "Avg #nodes:  39.9\n",
      "Med #nodes:   28.0\n",
      "Avg #tokens: 9675.5\n",
      "Med #tokens: 9811.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 430
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T14:22:08.934223Z",
     "start_time": "2025-02-24T14:22:08.932179Z"
    }
   },
   "cell_type": "code",
   "source": "sample(5, 20, alpha=0.2)",
   "id": "11540ccf62cf7380",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 5, 0, 19, 8]"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 425
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T14:51:28.137329Z",
     "start_time": "2025-02-24T14:51:27.495318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate with the other metrics\n",
    "ranks = []\n",
    "hit_at_1s = []\n",
    "hit_at_5s = []\n",
    "hits_at_5s = []\n",
    "recall_at_20s = []\n",
    "\n",
    "for qa in tqdm(qa_with_train_prompts):\n",
    "    all_answer_nodes = qa['answer_ids']\n",
    "    first_answer_node = all_answer_nodes[0] \n",
    "    predicted_nodes = qa['info_nodes']\n",
    "    try:\n",
    "        idx = predicted_nodes.index(first_answer_node)\n",
    "        ranks.append(idx + 1)\n",
    "    except Exception:\n",
    "        ranks.append(float('inf'))\n",
    "    hit_at_1s.append( 1 if first_answer_node in predicted_nodes[:1] else 0)\n",
    "    hit_at_5s.append( 1 if first_answer_node in predicted_nodes[:5] else 0 )\n",
    "    recall_at_20s.append( len(set(predicted_nodes).intersection(all_answer_nodes)) / len(all_answer_nodes) )\n",
    "\n",
    "mrr = np.mean([1/rank for rank in ranks])\n",
    "avg_hit_at_1 = np.mean(hit_at_1s)\n",
    "avg_hit_at_5 = np.mean(hit_at_5s)\n",
    "avg_recall_at_20 = np.mean(recall_at_20s)\n",
    "print(f\"Hit@1:     {avg_hit_at_1:.3f}\\nHit@5:     {avg_hit_at_5:.3f}\\nRecall@20: {avg_recall_at_20:.3f}\\nMRR:       {mrr:.3f}\\ninv. MRR:  {1/mrr:.3f}\")"
   ],
   "id": "c427ccc5c02c9b8c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6162/6162 [00:00<00:00, 9684.80it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit@1:     0.015\n",
      "Hit@5:     0.401\n",
      "Recall@20: 0.644\n",
      "MRR:       0.172\n",
      "inv. MRR:  5.818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 432
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "37866e76d1fd4485"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
