{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-22T15:47:12.731415Z",
     "start_time": "2025-01-22T15:47:12.726220Z"
    }
   },
   "source": [
    "from neo4j import GraphDatabase\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "load_dotenv('db.env', override=True)\n",
    "NEO4J_URI = os.getenv('NEO4J_URI')\n",
    "NEO4J_USERNAME = os.getenv('NEO4J_USERNAME')\n",
    "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "NEO4J_URI"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bolt://localhost:7687'"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 355
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T15:47:14.015619Z",
     "start_time": "2025-01-22T15:47:13.255628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_name = 'meta-llama/Llama-3.1-8B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ],
   "id": "276a728cb5f1f334",
   "outputs": [],
   "execution_count": 356
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T15:47:14.021134Z",
     "start_time": "2025-01-22T15:47:14.018080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cypher2path(cypher_query: str) -> list[tuple[str, str, str, str]]:\n",
    "    #path = re.findall(r\"(?:\\(|-\\[)(x|r)(\\d):([^ \\)\\]]+)(?: \\{name: \\\"(.+)\\\"\\})?(?:\\)|\\]-)\", cypher_query)\n",
    "    path = re.findall(r\"(?:\\(|-\\[)(x|r)(\\d):([^ \\)\\]]+)(?: \\{name: \\\"([^\\\"]+)\\\"\\})?(?:\\)|\\]-)\", cypher_query)\n",
    "    return path\n",
    "\n",
    "def block2cypher(x_r: str, num: str, label_or_type: str, name: str) -> str:\n",
    "    if x_r == 'x':\n",
    "        prop_string = f\" {{name: \\\"{name}\\\"}}\" if name != '' else \"\"\n",
    "        return f\"(x{num}:{label_or_type}{prop_string})\"\n",
    "    elif x_r == 'r':\n",
    "        return f\"-[r{num}:{label_or_type}]-\"\n",
    "\n",
    "def path2cypher(path: list[tuple[str, str, str, str]]) -> str:\n",
    "    query = \"MATCH \"\n",
    "    for x_r, num, label_or_type, name in path:\n",
    "        if x_r == 'x' or x_r == 'r':\n",
    "            query += block2cypher(x_r, num, label_or_type, name)\n",
    "        elif x_r == '':\n",
    "            query += f\" RETURN x{num}.name as name\"\n",
    "    return query"
   ],
   "id": "8ddcc94f752ce2bd",
   "outputs": [],
   "execution_count": 357
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T15:47:16.335963Z",
     "start_time": "2025-01-22T15:47:16.046309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = torch.load('prime-data/cypher_queries/cyphers_dataset-424.pt', weights_only=False)\n",
    "q_embs = torch.load('prime-data/text-embeddings-ada-002/query/query_emb_dict.pt', weights_only=False)\n",
    "questions = torch.load('prime-data/questions.pt', weights_only=False)"
   ],
   "id": "163a5316ac308910",
   "outputs": [],
   "execution_count": 358
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T15:47:16.867713Z",
     "start_time": "2025-01-22T15:47:16.863522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def modified_query(cypher_query: str) -> str:\n",
    "    cypher_query = cypher_query.split(\" RETURN\")[0]\n",
    "    path = cypher2path(cypher_query)\n",
    "    _, last_num, _, last_name = path[-1]\n",
    "    tgt = 'x3' if last_num == '3' and last_name == '' else 'x2'\n",
    "    return f\"\"\"{cypher_query} RETURN DISTINCT\n",
    "                                     {tgt}.name AS name, \n",
    "                                     {tgt}.details AS details, \n",
    "                                     {tgt}.nodeId AS node_id,\n",
    "                                     vector.similarity.cosine({tgt}.textEmbedding, $questionEmbedding) AS similarity\n",
    "                               ORDER BY similarity DESC\"\"\"\n",
    "\n",
    "def format_pattern(cypher_query: str, fetched_name: str) -> str:\n",
    "    path = cypher2path(cypher_query)\n",
    "    x_r, num, label, name = path[-1]\n",
    "    if name == '': #src-tgt AND src-var-tgt\n",
    "        path[-1] = (x_r, num, label, fetched_name)\n",
    "    else: #src1-tgt-src2\n",
    "        x_r, num, label, name = path[2]\n",
    "        path[2] = (x_r, num, label, fetched_name)\n",
    "    return path2cypher(path).lstrip(\"MATCH \")\n",
    "\n",
    "def make_description(rec: dict, cypher_query = None) -> (str, float):\n",
    "    name = rec['name']\n",
    "    details = rec['details']\n",
    "    if cypher_query is not None:\n",
    "        pattern = format_pattern(cypher_query, name)\n",
    "    else:\n",
    "        pattern = \"No pattern\" #Idea: use cypher to find pattern for vector similar nodes!\n",
    "    return f\"\"\"Name: {name}\\nPattern: {pattern}\\nDescription: {details}\"\"\"\n",
    "\n",
    "def data_fetcher(cypher_queries, question_embedding): #generator which produces db outputs\n",
    "    with GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD)) as driver:\n",
    "        for cypher_query in cypher_queries:\n",
    "            db_query = modified_query(cypher_query)\n",
    "            res = driver.execute_query(db_query, parameters_={'questionEmbedding': question_embedding})\n",
    "            for rec in res.records:\n",
    "                yield rec, cypher_query\n",
    "                \n",
    "def data_fetcher_vector_sim(question_embedding: list[float], num_nodes: int, found_node_ids: list[int]):\n",
    "    with GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD)) as driver:\n",
    "        db_query = \"\"\"CALL db.index.vector.queryNodes('textEmbedding', $numNodes, $questionEmbedding) YIELD node AS node, score\n",
    "                      WHERE NOT node.nodeId IN $foundNodeIds\n",
    "                      RETURN node.name AS name, \n",
    "                             node.details AS details, \n",
    "                             node.nodeId AS node_id,\n",
    "                             score AS similarity\"\"\"\n",
    "        res = driver.execute_query(db_query, {'numNodes': num_nodes, 'questionEmbedding': question_embedding, 'foundNodeIds': found_node_ids})\n",
    "    for rec in res.records:\n",
    "        yield rec\n",
    "        \n",
    "def get_answer_names(answer_ids: list[int]):\n",
    "    with GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD)) as driver:\n",
    "        db_query = \"\"\"UNWIND $nodeIds AS nodeId \n",
    "                      MATCH (x:_Entity_ {nodeId: nodeId})\n",
    "                      RETURN x.name as name\"\"\"\n",
    "        res = driver.execute_query(db_query, {'nodeIds': answer_ids})\n",
    "        answer_names = [rec['name'] for rec in res.records]\n",
    "        return answer_names"
   ],
   "id": "b2c051934f5e6347",
   "outputs": [],
   "execution_count": 359
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T15:47:47.745574Z",
     "start_time": "2025-01-22T15:47:18.009983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MAX_TOKENS = 2048\n",
    "EXTRA_TOKENS_PER_NODE = 10 #To assure we don't hit the context-window max size\n",
    "VECTOR_SIM_RATE = .3\n",
    "MAX_VECTOR_SIM_NODES = 20\n",
    "\n",
    "data = {}\n",
    "\n",
    "for i, row in tqdm(enumerate(dataset['train']), total=len(dataset['train'])):\n",
    "    idx = row['idx']\n",
    "    question = row['question']\n",
    "    cypher_queries = row['cyphers']\n",
    "    q_emb = q_embs[idx].tolist()[0]\n",
    "    \n",
    "    num_tokens = 100\n",
    "    node_ids = []\n",
    "    node_texts_and_sims = []\n",
    "    for rec, cypher_query in data_fetcher(cypher_queries, question_embedding=q_emb):\n",
    "        node_text = make_description(rec, cypher_query)\n",
    "        num_new_tokens = len(tokenizer.tokenize(node_text)) + EXTRA_TOKENS_PER_NODE\n",
    "        if num_tokens + num_new_tokens > VECTOR_SIM_RATE*MAX_TOKENS:\n",
    "            break\n",
    "        else:\n",
    "            num_tokens += num_new_tokens\n",
    "            node_ids.append(rec['node_id'])\n",
    "            node_texts_and_sims.append((node_text, rec['similarity']))\n",
    "    node_texts_and_sims.sort(key=lambda x: x[1], reverse=True) #Order by similarity (most similar first) (but append vector-similar to the end)\n",
    "           \n",
    "    for rec in data_fetcher_vector_sim(q_emb, num_nodes=MAX_VECTOR_SIM_NODES, found_node_ids=node_ids):\n",
    "        node_text = make_description(rec)\n",
    "        num_new_tokens = len(tokenizer.tokenize(node_text)) + EXTRA_TOKENS_PER_NODE\n",
    "        if num_tokens + num_new_tokens > MAX_TOKENS:\n",
    "            break\n",
    "        else:\n",
    "            num_tokens += num_new_tokens\n",
    "            node_ids.append(rec['node_id'])\n",
    "            node_texts_and_sims.append((node_text, rec['similarity']))\n",
    "    \n",
    "    \n",
    "    node_texts, _ = zip(*node_texts_and_sims) if node_texts_and_sims else ([], [])\n",
    "    info = '\\n\\n'.join(node_texts)\n",
    "    \n",
    "    answer_names = get_answer_names(eval(questions['answer_ids'][idx]))\n",
    "    answer = '|'.join(answer_names)\n",
    "    \n",
    "    data[idx] = {'idx': idx, 'info': info, 'question': question, 'answer': answer}"
   ],
   "id": "23ef3f4f0d30ac2d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 219/219 [00:29<00:00,  7.37it/s]\n"
     ]
    }
   ],
   "execution_count": 360
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T15:47:47.779645Z",
     "start_time": "2025-01-22T15:47:47.750444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame.from_dict(data).T\n",
    "#train_idx = list(set(idx_split['train'].tolist()).intersection(set(cypher.keys())))\n",
    "#train_data = Dataset.from_dict({'idx': train_idx, 'question': [questions[idx] for idx in train_idx], 'cypher': [cypher[idx] for idx in train_idx]})\n",
    "train_data = Dataset.from_pandas(df)\n",
    "\n",
    "val_idx = [] #list(set(idx_split['val'].tolist()).intersection(set(cypher.keys())))\n",
    "val_data = Dataset.from_dict({'idx': val_idx, 'question': [questions[idx] for idx in val_idx], 'cypher': [cypher[idx] for idx in val_idx]})\n",
    "\n",
    "test_idx = [] #list(set(idx_split['test'].tolist()).intersection(set(cypher.keys())))\n",
    "test_data = Dataset.from_dict({'idx': test_idx, 'question': [questions[idx] for idx in test_idx], 'cypher': [cypher[idx] for idx in test_idx]})\n",
    "\n",
    "dataset = DatasetDict({'train': train_data, 'val': val_data, 'test': test_data})"
   ],
   "id": "3dc46c0a24b334d0",
   "outputs": [],
   "execution_count": 361
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T15:47:47.803497Z",
     "start_time": "2025-01-22T15:47:47.794953Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(dataset, 'prime-data/llm2-dataset_shorter.pt')",
   "id": "d2bc9e42d4d4981c",
   "outputs": [],
   "execution_count": 362
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1059e77344c9d8ea"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
